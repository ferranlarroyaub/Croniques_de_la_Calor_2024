{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79735e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a693ef",
   "metadata": {},
   "source": [
    "# Data processing. Trajectories\n",
    "\n",
    "This notebook contains the necessary steps to process and filter the meteorological geolocated data collected via Meotracker sensors during the thermal walks. The notebook also contains an example of application for a given dataframe (csv file).\n",
    "\n",
    "**Resume:** \n",
    "\n",
    "The first step is to keep only the informative and non-empty columns. In the second step, linear interpolation is applied to the data to resample all records to a uniform temporal resolution of one measurement per second. Then, in the third step, the data from two sensors mounted on the same cart are averaged at every timestamp. Finally, in the fourth and last step a new shifted temperature is computed by subtracting, at each record, the temperature recorded by the fixed-station sensor located at the starting (and ending) point of the trajectory. This is done to mitigate the effect of natural temperature variation over the course of the day. The same procedure is applied to the humidex (HDX), which combines the temperature and the relative humidity. The resulting dataset for each trajectory (thermal walk) has 14 columns.\n",
    "\n",
    "The fixed reference sensors at the starting/ending point of the walk are processed in the same way excluding step number 4 (thus each fixed sensor dataset has 10 columns instead of 14).\n",
    "\n",
    "**Index:**\n",
    "\n",
    "    1. Remove columns\n",
    "    2. Linear interpolation\n",
    "    3. Average two sensors of the same cart\n",
    "    4. Shifted temperatures and humidex\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbe3ae",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Remove columns\n",
    "\n",
    "Each individual dataset initially contained 28 columns. Most of these columns were empty or corresponded to functionalities not supported by the MeteoTracker sensor model we used. Therefore, we retained only the 10 relevant columns: timestamp, latitude, longitude, temperature, relative humidity, altitude, preassure, dew-point, humidex and speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3009e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(df):\n",
    "    '''\n",
    "    Function that removes irrelevant and empty columns from a given dataframe,\n",
    "    keeping only the 10 relevant ones.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with only the relevant columns\n",
    "    '''\n",
    "    keep_columns = ['Time', 'Lat', 'Lon', 'Temp[°C]', 'Hum[%]', 'Alt[m]',\n",
    "                    'Press[mbar]', 'DP[°C]', 'HDX[°C]', 'Speed[km/h]']\n",
    "    \n",
    "    \n",
    "    # Keep only the columns that are both in the DataFrame and in keep_columns\n",
    "    relevant_df = df[[col for col in keep_columns if col in df.columns]]\n",
    "    \n",
    "    return relevant_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad6da4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Linear interpolation\n",
    "Each individual dataset trajectory is linearly interpolated to have all records (from all columns) with constant periodicity of one second. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(df):\n",
    "    '''\n",
    "    Function that interpolates linearly the data to have each record separated by 1 second.\n",
    "    \n",
    "    It fills the time gaps of more than one second using linear interpolation\n",
    "    on the continuous numeric variables (temperature, humidity, etc.).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with a 'Time' column (datetime format)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Interpolated dataframe with 1-second frequency\n",
    "    '''\n",
    "    print(f'The original data-set has {len(df)} records')\n",
    "\n",
    "    # Ensure 'Time' is datetime\n",
    "    df['Time'] = pd.to_datetime(df['Time'])\n",
    "    \n",
    "    # Set 'Time' as index\n",
    "    df = df.set_index('Time')\n",
    "\n",
    "\n",
    "    # Resample to 1-second intervals and interpolate linearly\n",
    "    df = df.resample('1S').asfreq().interpolate(method='linear')\n",
    "\n",
    "    # Reset index to bring 'Time' back as a column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    print(f'After interpolating, the new data-set has {len(df)} records\\n')\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca570f07",
   "metadata": {},
   "source": [
    "## 3. Average two sensors of the same cart\n",
    "Given two datasaets corresponding to two sensors mounted on the same cart, we average the data of the overlapping time of both sensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_2_sensors(df1, df2):\n",
    "    '''\n",
    "    Function that computes the average of two synchronized sensor datasets from the same trolley.\n",
    "    \n",
    "    Steps:\n",
    "    1. Aligns both dataframes to the common time range (intersection).\n",
    "    2. Verifies they have the same number of records.\n",
    "    3. Computes the mean value of all shared numeric columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df1 (pd.DataFrame): First sensor dataframe, must contain 'Time' column.\n",
    "        df2 (pd.DataFrame): Second sensor dataframe, must contain 'Time' column.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Averaged dataframe over shared time interval.\n",
    "    '''\n",
    "    \n",
    "    # Ensure Time is datetime\n",
    "    df1['Time'] = pd.to_datetime(df1['Time'])\n",
    "    df2['Time'] = pd.to_datetime(df2['Time'])\n",
    "\n",
    "    # Find overlapping time range\n",
    "    start_time = max(df1['Time'].min(), df2['Time'].min())\n",
    "    end_time = min(df1['Time'].max(), df2['Time'].max())\n",
    "\n",
    "    print(f'Dataset 1: {len(df1)} records, from {df1[\"Time\"].min()} to {df1[\"Time\"].max()}')\n",
    "    print(f'Dataset 2: {len(df2)} records, from {df2[\"Time\"].min()} to {df2[\"Time\"].max()}')\n",
    "    print(f'Overlap: from {start_time} to {end_time}')\n",
    "\n",
    "    # Trim both dataframes to overlapping range\n",
    "    df1_trimmed = df1[(df1['Time'] >= start_time) & (df1['Time'] <= end_time)].reset_index(drop=True)\n",
    "    df2_trimmed = df2[(df2['Time'] >= start_time) & (df2['Time'] <= end_time)].reset_index(drop=True)\n",
    "\n",
    "    # Verify matching record count\n",
    "    if len(df1_trimmed) != len(df2_trimmed):\n",
    "        raise ValueError(\"Trimmed dataframes do not have the same number of records after alignment.\")\n",
    "\n",
    "    print(f'Both dataframes trimmed to {len(df1_trimmed)} synchronized records.\\n')\n",
    "\n",
    "    # Columns to average (excluding 'Time')\n",
    "    shared_columns = [col for col in df1_trimmed.columns if col in df2_trimmed.columns and col != 'Time']\n",
    "    \n",
    "    # Average data\n",
    "    df_avg = pd.DataFrame()\n",
    "    df_avg['Time'] = df1_trimmed['Time']\n",
    "    for col in shared_columns:\n",
    "        df_avg[col] = (df1_trimmed[col] + df2_trimmed[col]) / 2\n",
    "\n",
    "    return df_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7b044",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Shifted temperatures and humidex\n",
    "Two new columns are added for the temperature and two new columns for humidex. One column corresponds to the instantaneous temperature/humidex difference between the sensor trajectory and the fixed reference sensor. The second column is the shifted temperature/humidex adjusted by adding the average fixed sensor temperature/humidex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_temperatures_and_humidex(df1, df_fixed):\n",
    "    '''\n",
    "    Recalculates temperature and humidex metrics relative to a fixed reference sensor.\n",
    "\n",
    "    New columns added to `df1`:\n",
    "        - 'T-T_fixed':          Instantaneous temperature difference.\n",
    "        - 'T-T_fixed+<T>':      Temperature difference adjusted by average fixed sensor temperature.\n",
    "        - 'HDX-HDX_fixed':      Instantaneous humidex difference.\n",
    "        - 'HDX-HDX_fixed+<HDX>': Humidex difference adjusted by average fixed sensor humidex.\n",
    "    \n",
    "    Both input dataframes must have a 'Time' column and overlapping time ranges.\n",
    "\n",
    "    Parameters:\n",
    "        df1 (pd.DataFrame): Mobile sensor dataset.\n",
    "        df_fixed (pd.DataFrame): Fixed reference sensor dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified `df1` with new computed columns.\n",
    "    '''\n",
    "    \n",
    "    # Ensure time columns are datetime\n",
    "    df1['Time'] = pd.to_datetime(df1['Time'])\n",
    "    df_fixed['Time'] = pd.to_datetime(df_fixed['Time'])\n",
    "\n",
    "    # Get common time range\n",
    "    start_time = max(df1['Time'].min(), df_fixed['Time'].min())\n",
    "    end_time = min(df1['Time'].max(), df_fixed['Time'].max())\n",
    "\n",
    "    print(f'Dataset 1: {len(df1)} records, from {df1[\"Time\"].min()} to {df1[\"Time\"].max()}')\n",
    "    print(f'Dataset 2: {len(df_fixed)} records, from {df_fixed[\"Time\"].min()} to {df_fixed[\"Time\"].max()}')\n",
    "    print(f'Common time window: {start_time} to {end_time}')\n",
    "\n",
    "    # Trim both datasets to the common time range\n",
    "    df1 = df1[(df1['Time'] >= start_time) & (df1['Time'] <= end_time)].reset_index(drop=True)\n",
    "    df_fixed = df_fixed[(df_fixed['Time'] >= start_time) & (df_fixed['Time'] <= end_time)].reset_index(drop=True)\n",
    "\n",
    "    if len(df1) != len(df_fixed):\n",
    "        raise ValueError(\"Trimmed dataframes do not match in length. Cannot compute element-wise differences.\")\n",
    "\n",
    "    print(f'Both dataframes aligned with {len(df1)} matching records.\\n')\n",
    "\n",
    "    # Calculate differences and adjusted metrics\n",
    "    df1['T-T_fixed'] = df1['Temp[°C]'] - df_fixed['Temp[°C]']\n",
    "    df1['T-T_fixed+<T>'] = df1['T-T_fixed'] + df_fixed['Temp[°C]'].mean()\n",
    "\n",
    "    df1['HDX-HDX_fixed'] = df1['HDX[°C]'] - df_fixed['HDX[°C]']\n",
    "    df1['HDX-HDX_fixed+<HDX>'] = df1['HDX-HDX_fixed'] + df_fixed['HDX[°C]'].mean()\n",
    "\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d7eed",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Example with a dataset\n",
    "\n",
    "    - Date of the experiment: 10/07/2024\n",
    "    - Using two sensors (sensor 1 and sensor 2) mounted in cart number 1. \n",
    "    - Using one fixed reference cart with 2 sensors (sensor 15 and sensor 17)\n",
    "\n",
    "###  0. Read the data-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed292bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart 1 --> Sensor 1\n",
    "df_cart1_sensor1 = pd.read_csv('Data_Repository_Heat_Chronicles\\\\original_trajectories\\\\SP_10july2024_cart_1_sensor_1.csv')  # Read csv file\n",
    "df_cart1_sensor1.drop_duplicates(subset='Time', keep='first', inplace=True, ignore_index=True) # Remove possible duplicates\n",
    "df_cart1_sensor1['Time'] = pd.to_datetime(df_cart1_sensor1['Time'], format='%Y-%m-%dT%H:%M:%S%z') # correct format\n",
    "df_cart1_sensor1['Time'] = df_cart1_sensor1['Time'].dt.tz_localize(None) \n",
    "\n",
    "# Cart 1 --> Sensor 2\n",
    "df_cart1_sensor2 = pd.read_csv('Data_Repository_Heat_Chronicles\\\\original_trajectories\\\\SP_10july2024_cart_1_sensor_2.csv')\n",
    "df_cart1_sensor2.drop_duplicates(subset='Time', keep='first', inplace=True, ignore_index=True)\n",
    "df_cart1_sensor2['Time'] = pd.to_datetime(df_cart1_sensor2['Time'], format='%Y-%m-%dT%H:%M:%S%z') \n",
    "df_cart1_sensor2['Time'] = df_cart1_sensor2['Time'].dt.tz_localize(None)\n",
    "\n",
    "\n",
    "# Fixed cart --> sensor 15\n",
    "df_fixed_cart_sensor15 = pd.read_csv('Data_Repository_Heat_Chronicles\\\\original_trajectories\\\\SP_10july2024_fixed_cart_sensor_15.csv')\n",
    "df_fixed_cart_sensor15.drop_duplicates(subset='Time', keep='first', inplace=True, ignore_index=True)\n",
    "df_fixed_cart_sensor15['Time'] = pd.to_datetime(df_fixed_cart_sensor15['Time'], format='%Y-%m-%dT%H:%M:%S%z') \n",
    "df_fixed_cart_sensor15['Time'] = df_fixed_cart_sensor15['Time'].dt.tz_localize(None)\n",
    "\n",
    "\n",
    "# Fixed cart --> sensor 17\n",
    "df_fixed_cart_sensor17 = pd.read_csv('Data_Repository_Heat_Chronicles\\\\original_trajectories\\\\SP_10july2024_fixed_cart_sensor_17.csv')\n",
    "df_fixed_cart_sensor17.drop_duplicates(subset='Time', keep='first', inplace=True, ignore_index=True)\n",
    "df_fixed_cart_sensor17['Time'] = pd.to_datetime(df_fixed_cart_sensor17['Time'], format='%Y-%m-%dT%H:%M:%S%z') \n",
    "df_fixed_cart_sensor17['Time'] = df_fixed_cart_sensor17['Time'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad414b6",
   "metadata": {},
   "source": [
    "### 1. Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cart1_sensor1_remove = remove_columns(df_cart1_sensor1)\n",
    "df_cart1_sensor1_remove = remove_columns(df_cart1_sensor2)\n",
    "\n",
    "df_fixed_cart_sensor15_remove = remove_columns(df_fixed_cart_sensor15)\n",
    "df_fixed_cart_sensor17_remove = remove_columns(df_fixed_cart_sensor17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed41455",
   "metadata": {},
   "source": [
    "### 2. Linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cart1_sensor1_interpolated = linear_interpolation(df_cart1_sensor1_remove)\n",
    "df_cart1_sensor2_interpolated = linear_interpolation(df_cart1_sensor1_remove)\n",
    "\n",
    "df_fixed_cart_sensor15_interpolated = linear_interpolation(df_fixed_cart_sensor15_remove)\n",
    "df_fixed_cart_sensor17_interpolated = linear_interpolation(df_fixed_cart_sensor17_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ad92d",
   "metadata": {},
   "source": [
    "###  3. Average two sensors of the same trolley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cart1_interpolated_averaged = average_2_sensors(df_cart1_sensor1_interpolated,df_cart1_sensor2_interpolated)\n",
    "\n",
    "df_fixed_cart_interpolated_averaged = average_2_sensors(df_fixed_cart_sensor15_interpolated,df_fixed_cart_sensor17_interpolated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e86275",
   "metadata": {},
   "source": [
    "### 4. Shifted temperatures and humidex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cart1_interpolated_averaged_shifted = shifted_temperatures_and_humidex(df_cart1_interpolated_averaged,\n",
    "                                                                          df_fixed_cart_interpolated_averaged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971b5d8",
   "metadata": {},
   "source": [
    "### Save new data-frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cart1_interpolated_averaged_shifted.to_csv('Data_Repository_Heat_Chronicles\\\\processed_trajectories\\\\SP_10july2024_cart_1.csv',index=False)\n",
    "#df_fixed_cart_interpolated_averaged.to_csv('Data_Repository_Heat_Chronicles\\\\processed_trajectories\\\\SP_10july2024_fixed_cart.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
